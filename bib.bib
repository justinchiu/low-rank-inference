% KERNEL SP BIB

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  archivePrefix = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% kernel models
@inproceedings{rahimi2007rff,
  author={Ali Rahimi and Benjamin Recht},
  title={Random Features for Large-Scale Kernel Machines},
  year={2007},
  cdate={1167609600000},
  pages={1177-1184},
  url={https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf},
  booktitle={NIPS}
}

@inproceedings{song2010kernelhmm,
author = {Song, Le and Boots, Byron and Siddiqi, Sajid M. and Gordon, Geoffrey and Smola, Alex},
title = {Hilbert Space Embeddings of Hidden Markov Models},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {991–998},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{song2011kerneltree,
 author = {Song, Le and Xing, Eric and Parikh, Ankur},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {2708--2716},
 publisher = {Curran Associates, Inc.},
 title = {Kernel Embeddings of Latent Tree Graphical Models},
 url = {https://proceedings.neurips.cc/paper/2011/file/dc09c97fd73d7a324bdbfe7c79525f64-Paper.pdf},
 volume = {24},
 year = {2011}
}

@misc{song2011kernelbp,
      title={Kernel Belief Propagation}, 
      author={Le Song and Arthur Gretton and Danny Bickson and Yucheng Low and Carlos Guestrin},
      year={2011},
      eprint={1105.5592},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% citation for mom not handling misspecification
@inproceedings{ruffini2017mom,
 author = {Ruffini, Matteo and Rabusseau, Guillaume and Balle, Borja},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {1901--1911},
 publisher = {Curran Associates, Inc.},
 title = {Hierarchical Methods of Moments},
 url = {https://proceedings.neurips.cc/paper/2017/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf},
 volume = {30},
 year = {2017}
}


% sampled softmax
@InProceedings{blanc2018sampledsoftmax, title = {Adaptive Sampled Softmax with Kernel Based Sampling}, author = {Blanc, Guy and Rendle, Steffen}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {590--599}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/blanc18a/blanc18a.pdf}, url = {http://proceedings.mlr.press/v80/blanc18a.html} }

@inproceedings{rawat2019sampledsoftmax,
 author = {Rawat, Ankit Singh and Chen, Jiecao and Yu, Felix Xinnan X and Suresh, Ananda Theertha and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {13857--13867},
 publisher = {Curran Associates, Inc.},
 title = {Sampled Softmax with Random Fourier Features},
 url = {https://proceedings.neurips.cc/paper/2019/file/e43739bba7cdb577e9e3e4e42447f5a5-Paper.pdf},
 volume = {32},
 year = {2019}
}


% efficient attention kernels
@inproceedings{katharopoulos2020lineartransformer,
    author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},
    title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
    booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
    year = {2020}
}

@misc{wang2020linformer,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{shen2018linearattn,
  author    = {Zhuoran Shen and
               Mingyuan Zhang and
               Shuai Yi and
               Junjie Yan and
               Haiyu Zhao},
  title     = {Factorized Attention: Self-Attention with Linear Complexities},
  journal   = {CoRR},
  volume    = {abs/1812.01243},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.01243},
  archivePrefix = {arXiv},
  eprint    = {1812.01243},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-01243.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{choromanski2020performer,
      title={Rethinking Attention with Performers}, 
      author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
      year={2020},
      eprint={2009.14794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
peng2021rfa,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QtTKTdVrFBB}
}

% kernel misc
@article{jebara2004kernel,
author = {Jebara, Tony and Kondor, Risi and Howard, Andrew},
title = {Probability Product Kernels},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {819–844},
numpages = {26}
}

@inproceedings{tsai2019kernelattn,
    title = "Transformer Dissection: An Unified Understanding for Transformer{'}s Attention via the Lens of Kernel",
    author = "Tsai, Yao-Hung Hubert  and
      Bai, Shaojie  and
      Yamada, Makoto  and
      Morency, Louis-Philippe  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1443",
    doi = "10.18653/v1/D19-1443",
    pages = "4344--4353",
}

% hmm



@article{jaeger2000,
author = {Jaeger, Herbert},
title = {Observable Operator Models for Discrete Stochastic Time Series},
journal = {Neural Computation},
volume = {12},
number = {6},
pages = {1371-1398},
year = {2000},
doi = {10.1162/089976600300015411},

URL = { 
        https://doi.org/10.1162/089976600300015411
    
},
eprint = { 
        https://doi.org/10.1162/089976600300015411
    
}
,
    abstract = { A widely used class of models for stochastic systems is hidden Markov models. Systems that can be modeled by hidden Markov models are a proper subclass of linearly dependent processes, a class of stochastic systems known from mathematical investigations carried out over the past four decades. This article provides a novel, simple characterization of linearly dependent processes, called observable operator models. The mathematical properties of observable operator models lead to a constructive learning algorithm for the identification of linearly dependent processes. The core of the algorithm has a time complexity of O (N + nm3), where N is the size of training data, n is the number of distinguishable outcomes of observations, and m is model state-space dimension. }
}



% pcfg
@inproceedings{kim2019cpcfg,
    title = "Compound Probabilistic Context-Free Grammars for Grammar Induction",
    author = "Kim, Yoon  and
      Dyer, Chris  and
      Rush, Alexander",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1228",
    doi = "10.18653/v1/P19-1228",
    pages = "2369--2385",
}

@inproceedings{
shen2018prpn,
title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},
author={Yikang Shen and Zhouhan Lin and Chin-wei Huang and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkgOLb-0W},
}

@inproceedings{
shen2018ordered,
title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
author={Yikang Shen and Shawn Tan and Alessandro Sordoni and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1l6qiR5F7},
}

% structured prediction scale
@misc{weiss2012structured,
      title={Structured Prediction Cascades}, 
      author={David Weiss and Benjamin Sapp and Ben Taskar},
      year={2012},
      eprint={1208.3279},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{deng2020cascaded,
      title={Cascaded Text Generation with Markov Transformers}, 
      author={Yuntian Deng and Alexander M. Rush},
      year={2020},
      eprint={2006.01112},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{wang2020ain,
      title={AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network}, 
      author={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},
      year={2020},
      eprint={2009.08229},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wiseman2019amortized,
      title={Amortized Bethe Free Energy Minimization for Learning MRFs}, 
      author={Sam Wiseman and Yoon Kim},
      year={2019},
      eprint={1906.06399},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% split merge
@phdthesis{huang2011thesis,
  author       = {Zhongqiang Huang}, 
  title        = {Modeling Dependencies in Natural Languages with Latent Variables},
  school       = {University of Maryland},
  year         = 2011,
    url = {https://drum.lib.umd.edu/handle/1903/12295},
}

@article{petrov2006splitmerge,
author = {Petrov, Slav and Barrett, Leon and Thibaux, Romain and Klein, Dan},
title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
year = {2006},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1220175.1220230},
doi = {10.3115/1220175.1220230},
booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics},
pages = {433–440},
numpages = {8},
location = {Sydney, Australia},
series = {ACL-44}
}

@inproceedings{cohen2012lpcfg,
 author = {Collins, Michael and Cohen, Shay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {2519--2527},
 publisher = {Curran Associates, Inc.},
 title = {Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs},
 url = {https://proceedings.neurips.cc/paper/2012/file/a58149d355f02887dfbe55ebb2b64ba3-Paper.pdf},
 volume = {25},
 year = {2012}
}

% insideoutside
@inproceedings{insideOutside,
  added-at = {2012-11-10T20:26:24.000+0100},
  author = {Baker, J. K.},
  biburl = {https://www.bibsonomy.org/bibtex/2101b883e31b9a00e77ae367ef80143d9/jil},
  booktitle = {Speech Communication Papers for the 97th Meeting of the  Acoustical Society of America},
  editor = {Klatt, D. H. and Wolf, J. J.},
  interhash = {b32a645737512d22cb61647f07fcf91a},
  intrahash = {101b883e31b9a00e77ae367ef80143d9},
  keywords = {algorithm cfg em inside outside},
  pages = {547--550},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {Trainable grammars for speech recognition},
  year = 1979
}

% ptb
@article{ptb,
author = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
title = {Building a Large Annotated Corpus of English: The Penn Treebank},
year = {1993},
issue_date = {June 1993},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {19},
number = {2},
issn = {0891-2017},
journal = {Comput. Linguist.},
month = jun,
pages = {313–330},
numpages = {18}
}


@article{mikolov-2011,
  added-at = {2013-12-08T18:05:26.000+0100},
  author = {Mikolov, Tomas and Deoras, Anoop and Kombrink, Stefan and Burget, Lukás and Cernocký, Jan},
  biburl = {https://www.bibsonomy.org/bibtex/2fd6741007f85845d79cbca2761fabe10/prlz77},
  ee = {http://www.isca-speech.org/archive/interspeech_2011/i11_0605.html},
  keywords = {Empirical advanced and combination evaluation language modeling of techniques},
  pages = {605-608},
  publisher = {ISCA},
  timestamp = {2013-12-08T18:05:26.000+0100},
  title = {Empirical Evaluation and Combination of Advanced Language Modeling Techniques.},
  url = {http://dblp.uni-trier.de/db/conf/interspeech/interspeech2011.html#MikolovDKBC11},
  year = 2011
}


@article{merity2017awdlstm,
  author    = {Stephen Merity and
               Nitish Shirish Keskar and
               Richard Socher},
  title     = {Regularizing and Optimizing {LSTM} Language Models},
  journal   = {CoRR},
  volume    = {abs/1708.02182},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.02182},
  archivePrefix = {arXiv},
  eprint    = {1708.02182},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-02182.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tran2016hmm,
  author    = {Ke M. Tran and
               Yonatan Bisk and
               Ashish Vaswani and
               Daniel Marcu and
               Kevin Knight},
  title     = {Unsupervised Neural Hidden Markov Models},
  journal   = {CoRR},
  volume    = {abs/1609.09007},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.09007},
  archivePrefix = {arXiv},
  eprint    = {1609.09007},
  timestamp = {Sat, 28 Sep 2019 00:58:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/TranBVMK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{awdlstmdoc,
    title = "Direct Output Connection for a High-Rank Language Model",
    author = "Takase, Sho  and
      Suzuki, Jun  and
      Nagata, Masaaki",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1489",
    doi = "10.18653/v1/D18-1489",
    pages = "4599--4609",
    abstract = "This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also middle layers. This method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). Our proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to application tasks: machine translation and headline generation.",
}

@misc{transformerlm,
      title={Language Models with Transformers}, 
      author={Chenguang Wang and Mu Li and Alexander J. Smola},
      year={2019},
      eprint={1904.09408},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}


@misc{hsu2012spectral,
      title={A Spectral Algorithm for Learning Hidden Markov Models}, 
      author={Daniel Hsu and Sham M. Kakade and Tong Zhang},
      year={2012},
      eprint={0811.4413},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{stratos-etal-2013-spectral,
    title = "Spectral Learning of Refinement {HMM}s",
    author = "Stratos, Karl  and
      Rush, Alexander  and
      Cohen, Shay B.  and
      Collins, Michael",
    booktitle = "Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W13-3507",
    pages = "56--64",
}

@article{stratos-etal-2016-unsupervised,
    title = "Unsupervised Part-Of-Speech Tagging with Anchor Hidden {M}arkov Models",
    author = "Stratos, Karl  and
      Collins, Michael  and
      Hsu, Daniel",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1018",
    doi = "10.1162/tacl_a_00096",
    pages = "245--257",
    abstract = "We tackle unsupervised part-of-speech (POS) tagging by learning hidden Markov models (HMMs) that are particularly well-suited for the problem. These HMMs, which we call anchor HMMs, assume that each tag is associated with at least one word that can have no other tag, which is a relatively benign condition for POS tagging (e.g., {``}the{''} is a word that appears only under the determiner tag). We exploit this assumption and extend the non-negative matrix factorization framework of Arora et al. (2013) to design a consistent estimator for anchor HMMs. In experiments, our algorithm is competitive with strong baselines such as the clustering method of Brown et al. (1992) and the log-linear model of Berg-Kirkpatrick et al. (2010). Furthermore, it produces an interpretable model in which hidden states are automatically lexicalized by words.",
}

@inproceedings{cohen-etal-2013-experiments,
    title = "Experiments with Spectral Learning of Latent-Variable {PCFG}s",
    author = "Cohen, Shay B.  and
      Stratos, Karl  and
      Collins, Michael  and
      Foster, Dean P.  and
      Ungar, Lyle",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1015",
    pages = "148--157",
}

@article{clark-fijalkow-2020-consistent,
    title = "Consistent Unsupervised Estimators for Anchored {PCFG}s",
    author = {Clark, Alexander  and
      Fijalkow, Nathana{\"e}l},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.27",
    doi = "10.1162/tacl_a_00323",
    pages = "409--422",
    abstract = "Learning probabilistic context-free grammars (PCFGs) from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs that satisfy certain natural conditions including being anchored (Stratos et al., 2016). We proceed via a reparameterization of (top--down) PCFGs that we call a bottom{--}up weighted context-free grammar. We show that if the grammar is anchored and satisfies additional restrictions on its ambiguity, then the parameters can be directly related to distributional properties of the anchoring strings; we show the asymptotic correctness of a naive estimator and present some simulations using synthetic data that show that algorithms based on this approach have good finite sample behavior.",
}
@inproceedings{he2017efficient,
  title={Efficient correlated topic modeling with topic embedding},
  author={He, Junxian and Hu, Zhiting and Berg-Kirkpatrick, Taylor and Huang, Ying and Xing, Eric P},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={225--233},
  year={2017}
}
@article{steinwart2006explicit,
  title={An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels},
  author={Steinwart, Ingo and Hush, Don and Scovel, Clint},
  journal={IEEE Transactions on Information Theory},
  volume={52},
  number={10},
  pages={4635--4643},
  year={2006},
  publisher={IEEE}
}
@article{javidian2020hypergraph,
  title={On a hypergraph probabilistic graphical model},
  author={Javidian, Mohammad Ali and Wang, Zhiyu and Lu, Linyuan and Valtorta, Marco},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={88},
  number={9},
  pages={1003--1033},
  year={2020},
  publisher={Springer}
}
@inproceedings{huang2005better,
  title={Better k-best parsing},
  author={Huang, Liang and Chiang, David},
  booktitle={Proceedings of the Ninth International Workshop on Parsing Technology},
  pages={53--64},
  year={2005}
}
@incollection{klein2004parsing,
  title={Parsing and hypergraphs},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={New developments in parsing technology},
  pages={351--372},
  year={2004},
  publisher={Springer}
}
@article{zhou2006learning,
  title={Learning with hypergraphs: Clustering, classification, and embedding},
  author={Zhou, Dengyong and Huang, Jiayuan and Sch{\"o}lkopf, Bernhard},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={1601--1608},
  year={2006},
  publisher={Citeseer}
}
@article{chiang2020factor,
  title={Factor Graph Grammars},
  author={Chiang, David and Riley, Darcey},
  journal={arXiv preprint arXiv:2010.12048},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{aizerman67linearization,
  added-at = {2008-05-27T08:15:25.000+0200},
  author = {Aizerman, M. A. and Braverman, E. A. and Rozonoer, L.},
  biburl = {https://www.bibsonomy.org/bibtex/20ace13b80e1f80aba4a27cb2f1b04b5c/mgrani},
  booktitle = { Automation and Remote Control, },
  interhash = {4848b39ce0f837dc7b429b700526c5b1},
  intrahash = {0ace13b80e1f80aba4a27cb2f1b04b5c},
  keywords = {imported},
  number = { 25 },
  pages = { 821-837 },
  timestamp = {2008-05-27T08:15:48.000+0200},
  title = { Theoretical foundations of the potential function
  method in pattern recognition learning. },
  year = { 1964 }
}

@article{jakel2007linearization,
  title = {A Tutorial on Kernel Methods for Categorization},
  author = {J{\"a}kel, F. and Sch{\"o}lkopf, B. and Wichmann, FA.},
  journal = {Journal of Mathematical Psychology},
  volume = {51},
  number = {6},
  pages = {343-358},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  month = dec,
  year = {2007},
  month_numeric = {12}
}

@article{rrhmm,
  author    = {Sajid M. Siddiqi and
               Byron Boots and
               Geoffrey J. Gordon},
  title     = {Reduced-Rank Hidden Markov Models},
  journal   = {CoRR},
  volume    = {abs/0910.0902},
  year      = {2009},
  url       = {http://arxiv.org/abs/0910.0902},
  archivePrefix = {arXiv},
  eprint    = {0910.0902},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-0910-0902.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{rrpcfg,
  author    = {Guillaume Rabusseau and
               Borja Balle and
               Shay B. Cohen},
  title     = {Weighted Tree Automata Approximation by Singular Value Truncation},
  journal   = {CoRR},
  volume    = {abs/1511.01442},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.01442},
  archivePrefix = {arXiv},
  eprint    = {1511.01442},
  timestamp = {Mon, 13 Aug 2018 16:47:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RabusseauBC15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{svm,
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
title = {A Training Algorithm for Optimal Margin Classifiers},
year = {1992},
isbn = {089791497X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.library.cornell.edu/10.1145/130385.130401},
doi = {10.1145/130385.130401},
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {144–152},
numpages = {9},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}

@inproceedings{polyphonic,
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
title = {Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1881–1888},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{implicit-rank-reg,
  author    = {Sanjeev Arora and
               Nadav Cohen and
               Wei Hu and
               Yuping Luo},
  title     = {Implicit Regularization in Deep Matrix Factorization},
  journal   = {CoRR},
  volume    = {abs/1905.13655},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.13655},
  archivePrefix = {arXiv},
  eprint    = {1905.13655},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-13655.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dmc,
author = {Siddiqi, Sajid M. and Moore, Andrew W.},
title = {Fast Inference and Learning in Large-State-Space HMMs},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.library.cornell.edu/10.1145/1102351.1102452},
doi = {10.1145/1102351.1102452},
abstract = {For Hidden Markov Models (HMMs) with fully connected transition models, the three fundamental problems of evaluating the likelihood of an observation sequence, estimating an optimal state sequence for the observations, and learning the model parameters, all have quadratic time complexity in the number of states. We introduce a novel class of non-sparse Markov transition matrices called Dense-Mostly-Constant (DMC) transition matrices that allow us to derive new algorithms for solving the basic HMM problems in sub-quadratic time. We describe the DMC HMM model and algorithms and attempt to convey some intuition for their usage. Empirical results for these algorithms show dramatic speedups for all three problems. In terms of accuracy, the DMC model yields strong results and outperforms the baseline algorithms even in domains known to violate the DMC assumption.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {800–807},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}

@inproceedings{ffthmm,
 author = {Felzenszwalb, Pedro and Huttenlocher, Daniel and Kleinberg, Jon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis},
 url = {https://proceedings.neurips.cc/paper/2003/file/9407c826d8e3c07ad37cb2d13d1cb641-Paper.pdf},
 volume = {16},
 year = {2004}
}

@misc{dedieu2019learning,
      title={Learning higher-order sequential structure with cloned HMMs}, 
      author={Antoine Dedieu and Nishad Gothoskar and Scott Swingle and Wolfgang Lehrach and Miguel Lázaro-Gredilla and Dileep George},
      year={2019},
      eprint={1905.00507},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{yang2021pcfgs,
      title={PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols}, 
      author={Songlin Yang and Yanpeng Zhao and Kewei Tu},
      year={2021},
      eprint={2104.13727},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
kaleidoscope,
title={Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps},
author={Tri Dao and Nimit Sohoni and Albert Gu and Matthew Eichhorn and Amit Blonder and Megan Leszczynski and Atri Rudra and Christopher Ré},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkgrBgSYDS}
}

@misc{rtransformer,
      title={R-Transformer: Recurrent Neural Network Enhanced Transformer}, 
      author={Zhiwei Wang and Yao Ma and Zitao Liu and Jiliang Tang},
      year={2019},
      eprint={1907.05572},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{betalstm,
  author    = {Kyungwoo Song and
               JoonHo Jang and
               Seungjae Shin and
               Il{-}Chul Moon},
  title     = {Bivariate Beta {LSTM}},
  journal   = {CoRR},
  volume    = {abs/1905.10521},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10521},
  archivePrefix = {arXiv},
  eprint    = {1905.10521},
  timestamp = {Tue, 10 Mar 2020 10:29:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-10521.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{flow,
      title={Latent Normalizing Flows for Discrete Sequences}, 
      author={Zachary M. Ziegler and Alexander M. Rush},
      year={2019},
      eprint={1901.10548},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{nasmc,
      title={Neural Adaptive Sequential Monte Carlo}, 
      author={Shixiang Gu and Zoubin Ghahramani and Richard E. Turner},
      year={2015},
      eprint={1506.03338},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tsbn,
      title={Deep Temporal Sigmoid Belief Networks for Sequence Modeling},
      author={Zhe Gan and Chunyuan Li and Ricardo Henao and David Carlson and Lawrence Carin},
      year={2015},
      eprint={1509.07087},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{srnn,
      title={Sequential Neural Models with Stochastic Layers}, 
      author={Marco Fraccaro and Søren Kaae Sønderby and Ulrich Paquet and Ole Winther},
      year={2016},
      eprint={1605.07571},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{gunasekar2017implicit,
      title={Implicit Regularization in Matrix Factorization}, 
      author={Suriya Gunasekar and Blake Woodworth and Srinadh Bhojanapalli and Behnam Neyshabur and Nathan Srebro},
      year={2017},
      eprint={1705.09280},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{razin2020rank,
 author = {Razin, Noam and Cohen, Nadav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {21174--21187},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Regularization in Deep Learning May Not Be Explainable by Norms},
 url = {https://proceedings.neurips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{scaling-laws,
  author    = {Jared Kaplan and
               Sam McCandlish and
               Tom Henighan and
               Tom B. Brown and
               Benjamin Chess and
               Rewon Child and
               Scott Gray and
               Alec Radford and
               Jeffrey Wu and
               Dario Amodei},
  title     = {Scaling Laws for Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/2001.08361},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.08361},
  archivePrefix = {arXiv},
  eprint    = {2001.08361},
  timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nplm,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1137–1155},
numpages = {19}
}

@inproceedings{
video,
title={Scaling Autoregressive Video Models},
author={Dirk Weissenborn and Oscar Täckström and Jakob Uszkoreit},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJgsskrFwH}
}

@article{topics,
  title={Applications of Topic Models},
  author={Jordan L. Boyd-Graber and Yuening Hu and David Mimno},
  journal={Found. Trends Inf. Retr.},
  year={2017},
  volume={11},
  pages={143-296}
}

@misc{levine2018reinforcement,
      title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
      author={Sergey Levine},
      year={2018},
      eprint={1805.00909},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{zhukov2019cross,
  title={Cross-task weakly supervised learning from instructional videos},
  author={Zhukov, Dimitri and Alayrac, Jean-Baptiste and Cinbis, Ramazan Gokberk and Fouhey, David and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3537--3545},
  year={2019}
}
@article{fried2020learning,
  title={Learning to segment actions from observation and narration},
  author={Fried, Daniel and Alayrac, Jean-Baptiste and Blunsom, Phil and Dyer, Chris and Clark, Stephen and Nematzadeh, Aida},
  journal={arXiv preprint arXiv:2005.03684},
  year={2020}
}

@book{pgm,
author = {Koller, Daphne and Friedman, Nir},
title = {Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning},
year = {2009},
isbn = {0262013193},
publisher = {The MIT Press},
abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs. Adaptive Computation and Machine Learning series}
}

@article{cotter2011approx,
  author    = {Andrew Cotter and
               Joseph Keshet and
               Nathan Srebro},
  title     = {Explicit Approximations of the Gaussian Kernel},
  journal   = {CoRR},
  volume    = {abs/1109.4603},
  year      = {2011},
  url       = {http://arxiv.org/abs/1109.4603},
  archivePrefix = {arXiv},
  eprint    = {1109.4603},
  timestamp = {Mon, 13 Aug 2018 16:47:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1109-4603.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yu2010hidden,
  title={Hidden semi-Markov models},
  author={Yu, Shun-Zheng},
  journal={Artificial intelligence},
  volume={174},
  number={2},
  pages={215--243},
  year={2010},
  publisher={Elsevier}
}
@inproceedings{constrainedhmm,
 author = {Roweis, Sam},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {Constrained Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper/1999/file/84c6494d30851c63a55cdb8cb047fadd-Paper.pdf},
 volume = {12},
 year = {2000}
}

@article{music-trans,
  author    = {Cheng{-}Zhi Anna Huang and
               Ashish Vaswani and
               Jakob Uszkoreit and
               Noam Shazeer and
               Curtis Hawthorne and
               Andrew M. Dai and
               Matthew D. Hoffman and
               Douglas Eck},
  title     = {An Improved Relative Self-Attention Mechanism for Transformer with
               Application to Music Generation},
  journal   = {CoRR},
  volume    = {abs/1809.04281},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.04281},
  archivePrefix = {arXiv},
  eprint    = {1809.04281},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-04281.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chiu2020scaling,
    title = "Scaling Hidden {M}arkov Language Models",
    author = "Chiu, Justin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.103",
    doi = "10.18653/v1/2020.emnlp-main.103",
    pages = "1341--1349",
    abstract = "The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization. Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.",
}

@article{sequnet,
  author    = {Daniel Stoller and
               Mi Tian and
               Sebastian Ewert and
               Simon Dixon},
  title     = {Seq-U-Net: {A} One-Dimensional Causal U-Net for Efficient Sequence
               Modelling},
  journal   = {CoRR},
  volume    = {abs/1911.06393},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.06393},
  archivePrefix = {arXiv},
  eprint    = {1911.06393},
  timestamp = {Sat, 23 Jan 2021 01:12:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-06393.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{storn,
      title={Learning Stochastic Recurrent Networks}, 
      author={Justin Bayer and Christian Osendorfer},
      year={2015},
      eprint={1411.7610},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{dmm,
      title={Structured Inference Networks for Nonlinear State Space Models}, 
      author={Rahul G. Krishnan and Uri Shalit and David Sontag},
      year={2016},
      eprint={1609.09869},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{adv-interp,
  author    = {Xinyang Zhang and
               Ningfei Wang and
               Shouling Ji and
               Hua Shen and
               Ting Wang},
  title     = {Interpretable Deep Learning under Fire},
  journal   = {CoRR},
  volume    = {abs/1812.00891},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.00891},
  archivePrefix = {arXiv},
  eprint    = {1812.00891},
  timestamp = {Wed, 17 Jul 2019 17:08:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-00891.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhang2021LabelFA,
  title={Label flipping attacks against Naive Bayes on spam filtering systems},
  author={Hongpo Zhang and Ning Cheng and Yang Zhang and Zhanbo Li},
  journal={Applied Intelligence},
  year={2021},
  pages={1-12}
}

@article{adv-phmm,
author = {Liu, Xiaolei and Zhuo, Zhongliu and Du, Xiaojiang and Zhang, Xiaosong and Zhu, Qingxin and Guizani, Mohsen},
year = {2018},
month = {12},
pages = {},
title = {Adversarial Attacks Against Profile HMM Website Fingerprinting Detection Model},
volume = {54},
journal = {Cognitive Systems Research},
doi = {10.1016/j.cogsys.2018.12.005}
}

@inproceedings{belanger2012column,
 author = {Belanger, David and Passos, Alexandre and Riedel, Sebastian and McCallum, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {MAP Inference in Chains using Column Generation},
 url = {https://proceedings.neurips.cc/paper/2012/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{mcauley2009map,
  author    = {Julian J. McAuley and
               Tib{\'{e}}rio S. Caetano},
  title     = {Exact Inference in Graphical Models: is There More to it?},
  journal   = {CoRR},
  volume    = {abs/0910.3301},
  year      = {2009},
  url       = {http://arxiv.org/abs/0910.3301},
  eprinttype = {arXiv},
  eprint    = {0910.3301},
  timestamp = {Mon, 22 Jul 2019 19:11:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-0910-3301.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{thai2018embedded,
    title = "Embedded-State Latent Conditional Random Fields for Sequence Labeling",
    author = "Thai, Dung  and
      Ramesh, Sree Harsha  and
      Murty, Shikhar  and
      Vilnis, Luke  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 22nd Conference on Computational Natural Language Learning",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K18-1001",
    doi = "10.18653/v1/K18-1001",
    pages = "1--10",
}